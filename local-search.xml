<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文阅读(1)——TpuGraphs</title>
    <link href="/2025/11/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1-tpugraphs/"/>
    <url>/2025/11/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1-tpugraphs/</url>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/2308.13490">TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</a></p><p>Github仓库：<a href="https://github.com/google-research-datasets/tpu_graphs">google-research-datasets/tpu_graphs</a></p><p>论文分析：</p><p><img src="TpuGraph_RoPE_Page1.png" /> <img src="TpuGraph_RoPE_Page2.png" /> <img src="TpuGraph_RoPE_Page3.png" /> <img src="TpuGraph_RoPE_Page4.png" /> <img src="TpuGraph_RoPE_Page5.png" /> <img src="TpuGraph_RoPE_Page6.png" /> <img src="TpuGraph_RoPE_Page7.png" /> <img src="TpuGraph_RoPE_Page8.png" /> <img src="TpuGraph_RoPE_Page9.png" /> <img src="TpuGraph_RoPE_Page10.png" /> <img src="TpuGraph_RoPE_Page11.png" /> <img src="TpuGraph_RoPE_Page12.png" /> <img src="TpuGraph_RoPE_Page13.png" /> <img src="TpuGraph_RoPE_Page14.png" /> <img src="TpuGraph_RoPE_Page15.png" /> <img src="TpuGraph_RoPE_Page16.png" /> <img src="TpuGraph_RoPE_Page17.png" /> <img src="TpuGraph_RoPE_Page18.png" /> <img src="TpuGraph_RoPE_Page19.png" /> <img src="TpuGraph_RoPE_Page20.png" /> <img src="TpuGraph_RoPE_Page21.png" /> <img src="TpuGraph_RoPE_Page22.png" /> <img src="TpuGraph_RoPE_Page23.png" /> <img src="TpuGraph_RoPE_Page24.png" /> <img src="TpuGraph_RoPE_Page25.png" /> <img src="TpuGraph_RoPE_Page26.png" /></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TPU</tag>
      
      <tag>Compiler</tag>
      
      <tag>AutoTuner</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer位置编码(2)——RoPE</title>
    <link href="/2025/11/04/Transformer-position-embedding2/"/>
    <url>/2025/11/04/Transformer-position-embedding2/</url>
    
    <content type="html"><![CDATA[<h2 id="第一部分深入解析旋转位置编码rope">第一部分：深入解析旋转位置编码（RoPE）</h2><p>旋转位置编码（RoPE）代表了位置编码领域的一次范式转移，它通过一种新颖的<strong>几何视角</strong>统一了绝对位置编码和相对位置编码。自提出以来，RoPE因其优越的性能和良好的理论特性，迅速成为众多先进大语言模型（如Llama、PaLM、Qwen）的标准配置。</p><p><img src="image1.png" /></p><h3 id="概念框架通过旋转编码位置">1.1 概念框架：通过旋转编码位置</h3><p>RoPE的<strong>核心思想</strong>极其直观且优雅：它不再通过向量加法来注入位置信息，而是通过<strong>旋转</strong>查询（Q）和键（K）向量来实现。<strong>每个词元在序列中的绝对位置 <span class="math inline"><em>m</em></span> 对应一个特定的旋转角度</strong>。当Q和K向量根据它们各自的绝对位置 <span class="math inline"><em>m</em></span> 和 <span class="math inline"><em>n</em></span> 进行旋转后，它们之间的点积（即注意力分数的核心部分）将<strong>只依赖于它们的相对位置 <span class="math inline"><em>m</em> − <em>n</em></span></strong>。</p><p><img src="image2.png" /></p><p><img src="image3.webp" /></p><p>我们可以借助一个“时钟”的比喻来理解这个过程：</p><ol type="1"><li><strong>多指针时钟</strong>：将一个 <span class="math inline"><em>d</em></span> 维的嵌入向量看作由 <span class="math inline"><em>d</em>/2</span> 个<strong>二维平面</strong>上的指针（或复数）组成。<strong>每个指针代表一个维度对</strong>。</li><li><strong>不同的转速</strong>：每个指针（维度对）都<strong>以自己独特的速度（频率）</strong>旋转。低维度的指针转得快，高维度的指针转得慢。</li><li><strong>位置即角度</strong>：<strong>一个词元的绝对位置 <span class="math inline"><em>m</em></span> 决定了所有指针需要旋转的角度</strong>。具体来说，第 <span class="math inline"><em>i</em></span> 个指针的旋转角度是 <span class="math inline"><em>m</em> ⋅ <em>θ</em><sub><em>i</em></sub></span>，其中 <span class="math inline"><em>θ</em><sub><em>i</em></sub></span> 是该指针固有的转速。</li><li><strong>相对位置的体现</strong>：当计算位置为 <span class="math inline"><em>m</em></span> 的查询和位置为 <span class="math inline"><em>n</em></span> 的键之间的注意力时，我们实际上是在比较这两个“时钟”状态。由于<strong>点积运算对旋转具有不变性</strong>（两个向量一起旋转相同的角度，它们的点积不变），最终的计算结果只取决于两个时钟指针之间的<strong>角度差</strong>，这个差值正比于相对距离 <span class="math inline"><em>m</em> − <em>n</em></span>。</li></ol><p><img src="image4.png" /></p><p>这种多尺度的旋转机制是RoPE成功的关键。快速旋转的指针（低维，高频）能够精确地区分邻近的位置，为模型提供细粒度的局部信息。而缓慢旋转的指针（高维，低频）则能在很长的距离上保持独特的角度，从而为模型提供捕捉远距离依赖所需的粗粒度全局信息。这种设计避免了单一频率可能导致的“混叠”问题，即在某个周期后，不同位置的旋转状态变得无法区分。</p><h3 id="数学公式与推导">1.2 数学公式与推导</h3><p>为了严谨地理解RoPE，我们需要深入其数学推导。其目标是找到一个编码函数 <span class="math inline"><em>f</em>(<strong>x</strong>, <em>m</em>)</span>，它作用于词元嵌入 <span class="math inline"><strong>x</strong></span> 和其绝对位置 <span class="math inline"><em>m</em></span>，使得编码后的查询 <span class="math inline"><em>f</em>(<strong>q</strong>, <em>m</em>)</span> 和键 <span class="math inline"><em>f</em>(<strong>k</strong>, <em>n</em>)</span> 的内积<strong>只与</strong> <span class="math inline"><strong>q</strong></span>、<span class="math inline"><strong>k</strong></span> 以及它们的相对位置 <span class="math inline"><em>m</em> − <em>n</em></span> 有关： <span class="math display">⟨<em>f</em>(<strong>q</strong>, <em>m</em>), <em>f</em>(<strong>k</strong>, <em>n</em>)⟩ = <em>g</em>(<strong>q</strong>, <strong>k</strong>, <em>m</em> − <em>n</em>)</span> 最简洁的推导方式是借助复数。我们可以将一个 <span class="math inline"><em>d</em></span> 维的实数向量 <span class="math inline"><strong>x</strong> ∈ ℝ<sup><em>d</em></sup></span> 视为一个 <span class="math inline"><em>d</em>/2</span> 维的复数向量 <span class="math inline"><strong>x</strong> ∈ ℂ<sup><em>d</em>/2</sup></span>，其中每两个连续的实数分量 <span class="math inline">(<em>x</em><sub>2<em>i</em></sub>, <em>x</em><sub>2<em>i</em> + 1</sub>)</span> 组成一个复数 <span class="math inline"><em>x</em><sub><em>i</em></sub> = <em>x</em><sub>2<em>i</em></sub> + <em>j</em> ⋅ <em>x</em><sub>2<em>i</em> + 1</sub></span> 。</p><p>在复数域中，旋转可以通过乘以一个模为1的复数 <span class="math inline"><em>e</em><sup><em>j</em><em>θ</em></sup></span> 来实现。我们定义编码函数为：</p><p><span class="math display"><em>f</em>(<strong>x</strong>, <em>m</em>) = <strong>x</strong> ⊙ <em>e</em><sup><em>j</em><em>m</em><strong>θ</strong></sup></span> 其中 <span class="math inline">⊙</span> 表示逐元素相乘，<span class="math inline"><strong>θ</strong> = [<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, …, <em>θ</em><sub><em>d</em>/2</sub>]</span> 是一个包含不同旋转频率的向量。</p><p>现在，我们来计算编码后的查询和键的内积（在复数域中，内积定义为 <span class="math inline">⟨<em>a</em>, <em>b</em>⟩ = <em>a</em> ⋅ <em>b̄</em></span>，其中 <span class="math inline"><em>b̄</em></span> 是 <span class="math inline"><em>b</em></span> 的共轭）： <span class="math display">$$\begin{align*}\langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle&amp;= \operatorname{Re}\big( (\mathbf{q}\odot e^{jm\boldsymbol{\theta}})\cdot\overline{(\mathbf{k}\odot e^{jn\boldsymbol{\theta}})} \big) \\&amp;= \operatorname{Re}\big( (\mathbf{q}\odot e^{jm\boldsymbol{\theta}})\cdot(\overline{\mathbf{k}}\odot e^{-jn\boldsymbol{\theta}}) \big) \\&amp;= \operatorname{Re}\big( (\mathbf{q}\cdot\overline{\mathbf{k}})\odot e^{j(m-n)\boldsymbol{\theta}} \big)\end{align*}$$</span> 从最后的结果可以看出，内积的表达式中只包含了相对位置 <span class="math inline"><em>m</em> − <em>n</em></span>，而绝对位置 <span class="math inline"><em>m</em></span> 和 <span class="math inline"><em>n</em></span> 被完美地消除了。</p><p>将这个思想转换回实数域，对于向量中的每一对维度 <span class="math inline">(<em>x</em><sub>2<em>i</em></sub>, <em>x</em><sub>2<em>i</em> + 1</sub>)</span>，<strong>上述复数乘法等价于乘以一个2D旋转矩阵</strong>： <span class="math display">$$\begin{pmatrix} x'_{2i} \ x'_{2i+1} \end{pmatrix} = \begin{pmatrix} \cos(m\theta_i) &amp; -\sin(m\theta_i) \\ \sin(m\theta_i) &amp; \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} x_{2i} \ x_{2i+1} \end{pmatrix}$$</span> 对于整个 <span class="math inline"><em>d</em></span> 维向量，这个操作<strong>相当于左乘一个块对角矩阵</strong>，其中<strong>每个对角块都是一个2D旋转矩阵</strong>。</p><p><img src="image5.webp" /></p><p><img src="image6.webp" /></p><p>旋转的频率 <span class="math inline"><em>θ</em><sub><em>i</em></sub></span> 通常按照以下公式定义： <span class="math display"><em>θ</em><sub><em>i</em></sub> = base<sup> − 2(<em>i</em> − 1)/<em>d</em></sup></span> 其中 <span class="math inline"><em>d</em></span> 是嵌入维度中用于RoPE的部分（通常是每个注意力头的维度），而 <span class="math inline">base</span> 是一个超参数，原始论文中设为10000。这个公式确保了频率 <span class="math inline"><em>θ</em><sub><em>i</em></sub></span> 随着维度索引 <span class="math inline"><em>i</em></span> 的增加而几何递减，从而实现了前文所述的<strong>多尺度“时钟”效果</strong>。</p><h3 id="rope的关键特性与理论优势">1.3 RoPE的关键特性与理论优势</h3><p>RoPE之所以被广泛采用，得益于其几个核心的理论优势：</p><ol type="1"><li><strong>序列长度的灵活性</strong>：由于RoPE在注意力计算中有效地编码了相对位置，它对序列长度具有很强的泛化能力。模型在较短序列上训练后，可以直接应用于更长的序列，而不会像某些绝对位置编码方法那样出现性能急剧下降的情况。</li><li><strong>远程依赖的衰减特性</strong>：从数学上可以证明，经过RoPE旋转后的两个向量，它们的内积会随着相对距离 <span class="math inline">|<em>m</em> − <em>n</em>|</span> 的增大而衰减。这为模型提供了一个非常有用的归纳偏置：<strong>距离越近的词元通常关系越密切，注意力得分应该越高；反之，距离越远的词元关系可能越弱</strong>。</li><li><strong>与线性注意力的兼容性</strong>：这是RoPE相较于传统RPE的一个决定性优势。因为RoPE是对Q和K向量的独立预处理，它发生在点积注意力计算之前。这意味着它与任何旨在避免计算完整注意力矩阵的线性注意力或高效注意力机制都是<strong>完全兼容</strong>的。这使得RoPE能够应用于对计算效率要求极高的超长序列模型中。</li></ol><h3 id="现代大语言模型中的rope分析以llama-3的rope_theta为例">1.4 现代大语言模型中的RoPE分析：以Llama 3的<code>rope_theta</code>为例</h3><p>理论的优雅需要通过实践来验证和优化。RoPE在现代大语言模型中的应用，特别是Llama 3系列模型的实现，为我们提供了一个绝佳的案例，展示了理论属性如何根据实际需求被调整。</p><p>在Llama 3的配置文件中，一个关键的超参数<code>rope_theta</code>被设置为500000.0，远大于原始RoPE论文中使用的10000.0。这个参数正是前文频率公式中的<span class="math inline">base</span>项。</p><p>这一调整并非无心之举，它深刻地改变了RoPE的行为。增大<code>rope_theta</code>的值，会使得所有的旋转频率 <span class="math inline"><em>θ</em><sub><em>i</em></sub></span> 变小。换言之，它让所有维度的“时钟指针”都转得更慢了。其直接后果是，RoPE固有的“远程依赖衰减”特性被显著减弱。当旋转角度随位置增长得更慢时，<strong>即使相对距离 <span class="math inline">|<em>m</em> − <em>n</em>|</span> 很大，旋转带来的相位差也较小，从而使得内积（注意力分数）的衰减变得平缓</strong>。</p><p>这一改动背后的动机是显而易见的。随着大模型竞赛进入“长上下文”时代，模型需要处理越来越长的文档、代码库或对话历史。在这些场景下，<strong>相距非常遥远的两个词元之间可能存在着至关重要的联系（例如，“大海捞针”测试）。在这种情况下，RoPE原始的快速衰减特性反而成了一种阻碍，因为它会惩罚模型对远距离信息的关注</strong>。</p><p>因此，Llama 3的工程师们通过调整<code>rope_theta</code>这一简单的杠杆，有原则地修改了模型的归纳偏置，使其更适应长上下文推理的任务。这完美地说明了RoPE的特性既是其优点，也是一个可调节的参数。理论上被认为是“理想属性”的远程衰减，在新的应用需求下，可以被灵活地调整甚至抑制。这突显了在最先进的模型开发中，深刻的理论理解与务实的工程决策之间相辅相成的关系。</p><h2 id="第二部分将rope扩展至多模态环境">第二部分：将RoPE扩展至多模态环境</h2><p>RoPE在处理一维文本序列方面取得了巨大成功，但真实世界的信息远不止于此。图像、视频等多模态数据具有更复杂的结构，这对位置编码提出了新的挑战。<strong>将RoPE从一维扩展到多维是构建像Qwen3-VL这样的高级视觉语言模型的关键一步</strong>。</p><h3 id="多维度的挑战从一维文本到二维视觉">2.1 多维度的挑战：从一维文本到二维视觉</h3><p>文本本质上是一维序列，而图像是二维的空间结构，<strong>视频则是在二维空间基础上增加了第三个维度——时间</strong>。简单地将图像分割成块（patches）然后展平成一个一维序列，虽然是Vision Transformer（ViT）的初始做法，但这种方式会丢失关键的几何信息。模型将无法理解“上方”、“左侧”或“右下角”等基本的空间关系，这对于需要细粒度视觉理解的任务（如目标检测、文档布局分析）是致命的。</p><p>因此，一个适用于多模态数据的位置编码方案，必须能够同时处理多个坐标轴（例如，时间 <span class="math inline"><em>t</em></span>、高度 <span class="math inline"><em>h</em></span>、宽度 <span class="math inline"><em>w</em></span>），并让模型理解这些维度上的相对关系。</p><h3 id="多模态ropem-rope位置设计与频率分配">2.2 多模态RoPE（M-RoPE）：位置设计与频率分配</h3><p>为了将RoPE扩展到多维，研究者们提出了多模态RoPE（M-RoPE）的概念。其核心思想是为每个维度（如 <span class="math inline"><em>t</em>, <em>h</em>, <em>w</em></span>）应用独立的旋转。然而，这一看似简单的扩展引入了两个关键且复杂的设计抉择：</p><ol type="1"><li><strong>位置设计（Position Design）</strong>：如何为文本和视觉（图像/视频帧）词元分配多维坐标 <span class="math inline">(<em>t</em>, <em>h</em>, <em>w</em>)</span>。一个好的设计必须能够明确区分不同模态的词元，并避免位置歧义。例如，一些早期方案可能会导致视觉词元和后续生成的文本词元拥有重叠的坐标，造成“模态混淆”。</li><li><strong>频率分配（Frequency Allocation）</strong>：如何将模型的嵌入维度（例如，一个注意力头的128维）分配给不同的坐标轴（<span class="math inline"><em>t</em>, <em>h</em>, <em>w</em></span>）来计算旋转。这是M-RoPE实现中最具挑战性也最关键的一环。</li></ol><p>一种直接的频率分配策略是<strong>“分块式”（Blocked）</strong>分配。例如，对于一个128维的向量，可以将前42维分配给时间轴 <span class="math inline"><em>t</em></span>，接下来的43维分配给高度轴 <span class="math inline"><em>h</em></span>，最后43维分配给宽度轴 <span class="math inline"><em>w</em></span>。每个轴在其分配到的维度块内，独立地应用从高到低的旋转频率。这种方法虽然简单，但存在严重缺陷：<strong>它迫使某些轴（在这个例子中是时间轴 <span class="math inline"><em>t</em></span>）只能使用高频旋转</strong>，而其他轴只能使用中低频旋转。高频旋转意味着注意力会随着距离的增加而迅速衰减，这对于需要<strong>捕捉长时序关系的视频理解任务</strong>是极其不利的。同时，不同轴使用<strong>不对称</strong>的频率范围也可能引入不必要的偏见。</p><p>下表对比了不同的M-RoPE设计策略，系统地展示了它们各自的局限性，并为理解Qwen3-VL所采用的“交错式”方案的动机提供了背景。</p><table><thead><tr class="header"><th><strong>设计变体</strong></th><th><strong>位置分配策略</strong></th><th><strong>频率分配策略</strong></th><th><strong>主要局限性</strong></th></tr></thead><tbody><tr class="odd"><td><strong>一维序列化</strong></td><td>将所有词元展平为一维序列</td><td>所有维度用于单一轴</td><td>丢失2D/3D几何结构，空间推理能力差</td></tr><tr class="even"><td><strong>MRoPE (分块式)</strong></td><td>为视觉词元分配3D坐标</td><td>将嵌入维度<strong>分块</strong>分配给t, h, w轴</td><td>频率分配不均，导致某些轴（如时间）注意力快速衰减，不利于长时序建模</td></tr><tr class="odd"><td><strong>VideoRoPE (对角线布局)</strong></td><td>对角线式分配位置ID</td><td>将时间轴分配给低频维度</td><td>可能导致视觉和文本词元位置重叠，引发模态混淆</td></tr><tr class="even"><td><strong>Interleaved-MRoPE (交错式)</strong></td><td>为视觉词元分配3D坐标</td><td>将嵌入维度<strong>交错</strong>分配给t, h, w轴</td><td>解决了频率分配不均的问题，为所有轴提供全频谱频率覆盖</td></tr></tbody></table><h3 id="交错式rope变体频率分配的范式转变">2.3 交错式RoPE变体：频率分配的范式转变</h3><p><img src="image7.jpg" /></p><p>为了克服分块式分配的缺陷，一种更先进的策略——<strong>“交错式”（Interleaved）</strong>频率分配——被提出并应用于Qwen3-VL等模型中。</p><p>交错式分配的核心思想是，不再将连续的维度块分配给单个轴，而是在所有轴之间以<strong>轮询（round-robin）的方式</strong>交错分配维度。以一个三维坐标 <span class="math inline">(<em>t</em>, <em>h</em>, <em>w</em>)</span> 为例，分配方式可能如下：</p><ul><li>维度对0分配给时间轴 <span class="math inline"><em>t</em></span></li><li>维度对1分配给高度轴 <span class="math inline"><em>h</em></span></li><li>维度对2分配给宽度轴 <span class="math inline"><em>w</em></span></li><li>维度对3分配给时间轴 <span class="math inline"><em>t</em></span></li><li>维度对4分配给高度轴 <span class="math inline"><em>h</em></span></li><li>… 以此类推</li></ul><p>通过这种方式，<strong>每一个坐标轴（<span class="math inline"><em>t</em>, <em>h</em>, <em>w</em></span>）都能获得从最高到最低的完整频率谱</strong>。时间轴 <span class="math inline"><em>t</em></span> 不再局限于高频维度，它同样可以使用低频维度来进行缓慢旋转，从而有效捕捉长距离的时间依赖。同理，<strong>空间轴 <span class="math inline"><em>h</em></span> 和 <span class="math inline"><em>w</em></span> 也能利用高频维度来感知局部细节，利用低频维度来理解全局布局</strong>。</p><p>这种方法从根本上解决了分块式分配导致的注意力快速衰减和不对称衰减的问题，为所有维度提供了均衡且鲁棒的多尺度建模能力。这种实现模式在一些底层算子库中也有体现，例如ONNX的<code>RotaryEmbedding</code>算子就提供了一个<code>interleaved</code>属性来支持这种计算模式，这表明它是一种被业界认可的高效实现方案。</p><h2 id="第三部分解构qwen3-vl架构中的rope">第三部分：解构Qwen3-VL架构中的RoPE</h2><p>本部分将综合前述所有理论知识，对目标模型Qwen3-VL中的RoPE实现进行一次集中的、深入的分析。我们将追溯其在Qwen系列中的演进，并详细剖析其“交错式M-RoPE”的具体实现及其对模型性能的深远影响。</p><h3 id="qwen-vl系列中位置编码的架构概览">3.1 Qwen-VL系列中位置编码的架构概览</h3><p>Qwen（通义千问）系列模型在位置编码的选择上经历了一个清晰的演进过程，反映了该领域技术发展的趋势。早期的Qwen-VL模型，在其视觉-语言适配器中采用了二维绝对位置编码来保留图像特征的位置信息，这是一种相对传统和直接的方法。然而，随着模型向更复杂的时空理解能力发展，后续的模型版本，特别是Qwen2-VL和Qwen3-VL，转向了更先进的RoPE方案，并专门为其多模态特性设计了M-RoPE。这一转变标志着模型设计从简单的位置注入转向了通过几何变换来编码复杂的相对关系，旨在获得更强的泛化和外推能力。</p><h3 id="qwen3-vl中的交错式m-rope实现">3.2 Qwen3-VL中的交错式M-RoPE实现</h3><p>Qwen3-VL模型架构的一项核心更新，被官方明确描述为<strong>“增强的、采用交错式布局的MRoPE” (enhanced MRope with interleaved layout)</strong> 和 <strong>“交错式MRoPE” (Interleaved-MRoPE)</strong>。这正是前面部分所讨论的先进频率分配方案。</p><p>Qwen团队在其<a href="%5BQwen%5D(https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list)">技术博客</a>中明确指出了做出这一改变的原因：他们识别出原始“分块式”MRoPE将所有时序信息集中在高频维度的缺陷，并采用交错式方案来实现<strong>“跨越时间、高度和宽度的全频率覆盖”</strong>。这一决策是经过深思熟虑的，旨在直接解决前代M-RoPE方案的瓶颈。</p><p>通过分析Qwen3-VL在Hugging Face等平台的官方代码库（例如<code>modeling_qwen3_vl.py</code>文件）和文档，我们可以进一步理解其实现细节。在代码层面，模型会为输入的视觉词元（来自图像或视频帧的patches）生成三维坐标 <span class="math inline">(<em>t</em>, <em>h</em>, <em>w</em>)</span>。在应用RoPE时，查询和键向量的嵌入维度会根据交错模式被逻辑上地分配给这三个轴。对于一个给定的维度对，它会被指定用于旋转其中一个轴的坐标，而这个指定关系会在所有维度对上循环。最终，每个词元的Q和K向量会经历三次独立的旋转（分别由<span class="math inline"><em>t</em>, <em>h</em>, <em>w</em></span>坐标驱动），这些旋转效果叠加在一起，形成最终的位置编码。这种方式确保了模型在计算注意力时，能够同时、均衡地考虑所有维度上的相对位置关系。</p><h3 id="实践意义与性能分析">3.3 实践意义与性能分析</h3><p>Qwen3-VL采用交错式M-RoPE并非单纯的学术探索，而是由解决实际问题驱动的工程决策。官方文档和技术报告反复强调，这一特定的实现带来了<strong>“更好的时空建模能力”</strong>和<strong>“增强的长时程视频推理能力”</strong>。</p><p>这背后的因果联系是清晰且直接的：</p><ol type="1"><li><strong>提升长视频理解能力</strong>：通过确保时间轴 <span class="math inline"><em>t</em></span> 能够使用低频旋转，模型的注意力不会随着视频时长的增加而过快衰减。这使得模型能够有效地关联视频开头和结尾的关键事件，从而在长达数小时的视频中实现精准的事件定位和内容理解。</li><li><strong>增强空间推理能力</strong>：通过确保空间轴 <span class="math inline"><em>h</em></span> 和 <span class="math inline"><em>w</em></span> 也能访问完整的频率谱，模型获得了真正的多尺度空间感知能力。高频旋转使其能够关注图像中的精细纹理和微小物体（如文档中的字符），而低频旋转使其能够理解整体的布局和场景结构（如表格的行列关系）。</li></ol><p>这种架构上的优化，直接转化为模型在各项基准测试和实际应用中的卓越表现。Qwen3-VL在复杂的长视频问答、文档解析和视觉定位等任务上的领先性能，很大程度上归功于其先进且经过精心设计的交错式M-RoPE机制。</p><p>总而言之，Qwen3-VL中交错式M-RoPE的应用，是复杂问题驱动架构演进的典范。它并非一项全新的发明，而是对现有技术（M-RoPE）的深刻理解和精巧改良。这反映出大模型研发已进入一个成熟阶段，性能的提升不再仅仅依赖于数据和参数规模的堆砌，更来自于对模型内部机制的深刻洞察和有针对性的架构优化。这个过程可以概括为：识别关键任务瓶颈（如长视频理解）-&gt; 分析底层架构局限（如分块式M-RoPE的频率分配问题）-&gt; 提出并实施针对性的解决方案（交错式分配）-&gt; 实现模型能力的代际跃升。</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer位置编码(1)——初探</title>
    <link href="/2025/11/03/Transformer-position-embedding1/"/>
    <url>/2025/11/03/Transformer-position-embedding1/</url>
    
    <content type="html"><![CDATA[<h2 id="第一部分transformer中位置信息的基础原理">第一部分：Transformer中位置信息的基础原理</h2><h3 id="自注意力机制中的置换不变性问题">1.1 自注意力机制中的置换不变性问题</h3><p>Transformer架构的核心是<strong>自注意力（Self-Attention）机制</strong>，它赋予了模型强大的并行处理能力和捕捉长距离依赖的潜力。然而，这种设计也带来了一个固有的、必须被正视的局限性：<strong>置换不变性（Permutation Invariance）</strong>。从根本上说，自注意力机制将输入序列视为一个<strong>无序的标记集合（“bag of words”）</strong>，而非一个有序的序列。</p><p>这种特性源于其核心计算过程。对于一个给定的输入序列，自注意力机制通过计算查询（Query, Q）、键（Key, K）和值（Value, V）矩阵之间的点积来生成注意力分数，其公式可以简化为 <span class="math inline">$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</span>。在这个计算中，任何一个标记的输出表示都是其对序列中所有其他标记的加权求和。关键在于，这些权重仅取决于Q和K向量之间的相似度，而与它们在序列中的原始位置无关。因此，如果输入序列的顺序被打乱，自注意力机制的输出序列也会以同样的方式被打乱，但每个标记自身的表示向量并不会因其位置的改变而改变。这一特性被称为置换等变性（Permutation Equivariance）。</p><p>这种架构特性意味着，如果没有额外的位置信息，模型将无法区分语义完全不同的句子，例如“猫坐在垫子上”和“垫子坐在猫上”。对于模型而言，这两个句子包含了完全相同的词元集合，因此自注意力机制会为它们生成相似的内部表示，这在自然语言处理任务中是不可接受的。</p><p>深入分析可以发现，对位置编码的需求并非Transformer设计中的疏忽，而是其最大优势——<strong>并行化</strong>——所带来的必然结果。传统的序列模型，如循环神经网络（RNN）和长短期记忆网络（LSTM），通过其固有的循环结构来处理序列。它们一次处理一个词元，将前一个时间步的信息通过隐藏状态传递到下一个时间步，这种顺序处理的方式天然地编码了词元的位置和顺序信息。然而，这种顺序依赖性也成为了它们的瓶颈，阻碍了大规模并行计算，限制了模型在现代硬件（如GPU）上的训练效率。</p><p>Transformer架构通过完全摒弃循环结构，实现了对整个输入序列的并行处理，从而极大地提升了计算效率。模型中的每个词元可以同时与所有其他词元进行交互，这使得Transformer能够高效地处理长序列。然而，正是这种架构选择，即用并行计算换取顺序处理，导致了模型失去了对序列顺序的内在感知能力。因此，<strong>位置编码（Positional Encoding）</strong>应运而生，它是一种工程上的解决方案，旨在将至关重要的顺序信息重新注入到一个本质上是并行化的框架中。这体现了在计算效率和结构感知能力之间经典的工程权衡。</p><p><img src="image1.jpg" /></p><h3 id="位置编码策略分类">1.2 位置编码策略分类</h3><p>为了解决自注意力机制的置换不变性问题，研究者们开发了多种策略来为模型提供位置信息。这些策略可以大致分为<strong>三个主要类别</strong>，为理解后续更高级的技术（如RoPE）提供了清晰的框架。</p><ol type="1"><li><strong>绝对位置编码 (Absolute Positional Embeddings, APE):</strong> 这类方法为序列中的<strong>每一个绝对位置</strong>（例如，第1个位置、第2个位置等）<strong>分配一个唯一的、固定的向量</strong>。这个位置向量随后会与对应位置的词元嵌入（token embedding）相结合，通常是通过相加的方式。这样，即使是相同的词元，在序列的不同位置也会拥有不同的最终表示，从而使模型能够区分它们的顺序。最经典的方法是Vaswani等人在原始Transformer论文中提出的正弦/余弦编码。</li><li><strong>相对位置编码 (Relative Positional Embeddings, RPE):</strong> 与关注绝对位置不同，相对位置编码的核心思想是模型更应该关注词元之间的相对关系，即<strong>它们之间的距离或方向</strong>。这类方法通常不是在输入层修改词元嵌入，而是在自注意力计算过程中直接注入相对位置信息。例如，通过为查询和键之间的特定距离添加一个可学习的偏置项（bias），使得注意力分数能够感知到两个词元“相距多远”。</li><li><strong>混合/旋转方法 (Hybrid/Rotary Approaches): </strong>这类方法，以旋转位置编码（Rotary Position Embedding, RoPE）为代表，巧妙地<strong>结合了绝对位置和相对位置编码的思想</strong>。RoPE利用一个与绝对位置相关的函数（旋转矩阵）来<strong>变换查询Q和键向量K</strong>。其精妙之处在于，经过变换后的查询和键向量进行点积运算时，其<strong>结果仅依赖于它们的相对位置</strong>。这样，模型既利用了绝对位置信息来生成编码，又在注意力机制中实现了纯粹的相对位置依赖。</li></ol><h2 id="第二部分向相对位置感知的演进">第二部分：向相对位置感知的演进</h2><p>位置编码技术的发展历程反映了研究界对模型如何最有效地利用序列信息的不断深入的理解。从最初的绝对位置方案，到更加灵活的相对位置方案，每一次演进都是为了克服前代方法的局限性。</p><h3 id="绝对位置编码ape正弦函数方法">2.1 绝对位置编码（APE）：正弦函数方法</h3><p>在最初的Transformer论文《Attention Is All You Need》中，Vaswani等人提出了一种优雅且高效的绝对位置编码方案，即正弦位置编码。该方法不依赖于学习，而是通过一个固定的数学公式生成位置向量。</p><p>其核心公式如下： <span class="math display">$$PE_{(pos, 2i)} = \sin(\frac{pos}{10000^{2i/d_{\text{model}}}})$$</span></p><p><span class="math display">$$PE_{(pos, 2i+1)} = \cos(\frac{pos}{10000^{2i/d_{\text{model}}}})$$</span></p><p>其中：</p><ul><li><span class="math inline"><em>p</em><em>o</em><em>s</em></span> 是词元在序列中的绝对位置索引（从0开始）。</li><li><span class="math inline"><em>i</em></span> 是位置编码向量中的维度索引（从0开始）。</li><li><span class="math inline"><em>d</em><sub>model</sub></span> 是词元嵌入和位置编码向量的总维度。</li></ul><p><img src="image2.png" /></p><p>这个公式的设计蕴含了深刻的考量。首先，通过<strong>交替使用正弦和余弦函数</strong>，并为每个维度对（<span class="math inline">2<em>i</em></span> 和 <span class="math inline">2<em>i</em> + 1</span>）分配一个不同的波长（频率），它为每个位置 <span class="math inline"><em>p</em><em>o</em><em>s</em></span> 生成了一个唯一的 <span class="math inline"><em>d</em><sub>model</sub></span> 维向量。这些波长的变化范围从短到长：对于维度索引 <span class="math inline"><em>i</em></span> 较小的<strong>低维部分</strong>（例如，<span class="math inline"><em>i</em> = 0, 1, 2...</span>，即编码向量的起始部分），分母中的指数项接近0，使得频率很大（接近1），对应的波长很短，函数值变化迅速。这种快速的变化为模型提供了非常精确的、关于<strong>相邻词元</strong>之间差异的信号。模型可以轻易地分辨出“这是第3个词”和“这是第4个词”；对于<strong>高维部分</strong>（例如，<span class="math inline"><em>i</em></span>接近<span class="math inline"><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub>/2</span>，即编码向量的末尾部分），频率则变得非常小，对应波长较大，那么函数值会随着<span class="math inline"><em>p</em><em>o</em><em>s</em></span>的增加而变得非常缓慢，只有当<span class="math inline"><em>p</em><em>o</em><em>s</em></span>的值发生巨大变化时（例如，从<span class="math inline"><em>p</em><em>o</em><em>s</em> = 5</span>到<span class="math inline"><em>p</em><em>o</em><em>s</em> = 500</span>），编码值才会有明显的不同，从而使模型把握到全局信息。这种多尺度的频率组合使得模型能够同时捕捉到粗粒度和细粒度的位置信息。</p><p><img src="image3.jpg" /></p><p>其次，这种基于正弦函数的设计有一个重要的特性：对于任意固定的偏移量 <span class="math inline"><em>k</em></span>，位置 <span class="math inline"><em>p</em><em>o</em><em>s</em> + <em>k</em></span> 的位置编码 <span class="math inline"><em>P</em><em>E</em><sub>(<em>p</em><em>o</em><em>s</em> + <em>k</em>)</sub></span> 可以表示为 <span class="math inline"><em>P</em><em>E</em><sub>(<em>p</em><em>o</em><em>s</em>)</sub></span> 的线性函数。这一特性使得模型能够更容易地学习到<strong>相对位置关系</strong>。模型可以通过注意力机制学习到如何关注相距特定距离的词元，因为它们的<strong>位置编码之间存在固定的线性变换关系</strong>。</p><p>最后，一个常被引用的优点是，这种确定性的公式使得模型理论上能够外推到比训练时遇到的更长的序列长度。因为即使对于未见过的位置 <span class="math inline"><em>p</em><em>o</em><em>s</em></span>，我们依然可以计算出其对应的位置编码向量（泛化能力更强）。生成的位置编码向量会与词元的语义嵌入向量逐元素相加，形成最终的输入表示，然后被送入Transformer的后续层。</p><p><img src="image4.png" /></p><h3 id="相对位置编码rpe的兴起">2.2 相对位置编码（RPE）的兴起</h3><p>尽管正弦绝对位置编码在实践中表现良好，但它也存在一些理论上的局限性。例如，虽然它允许模型学习相对关系，但这种关系是间接的。此外，对于非常长的序列，其外推能力的鲁棒性也受到质疑。为了更直接、更灵活地建模词元间的相对关系，相对位置编码（RPE）应运而生。</p><p>RPE的核心思想是，词元之间的注意力强度不应取决于它们的绝对位置，而应取决于它们之间的相对距离。例如，在句子中相距3个词元的两个词，无论它们出现在句首还是句末，它们之间的关系模式都可能具有共性。</p><p>与APE在输入层修改嵌入向量不同，RPE通常在自注意力机制的内部发挥作用。一种常见且有影响力的方法（如T5模型中所使用的）是，在计算注意力分数时，直接向查询-键（Query-Key）点积结果中添加一个可学习的偏置项（bias）。这个偏置项的值取决于查询和键之间的相对距离 <span class="math inline"><em>j</em> − <em>i</em></span>。</p><p>具体来说，注意力分数的计算可以修改为： <span class="math display">$$e_{ij} = \frac{(x_i W_Q)(x_j W_K)^T + b_{j-i}}{\sqrt{d_k}}$$</span> 其中，<span class="math inline"><em>x</em><sub><em>i</em></sub><em>W</em><sub><em>Q</em></sub></span> 是查询向量，<span class="math inline"><em>x</em><sub><em>j</em></sub><em>W</em><sub><em>K</em></sub></span> 是键向量，<span class="math inline"><em>b</em><sub><em>j</em> − <em>i</em></sub></span> 是一个与相对距离 <span class="math inline"><em>j</em> − <em>i</em></span> 对应的可学习标量偏置。通过为每个可能的相对距离（例如，-k到+k）学习一个独立的偏置值，模型可以直接建模相对位置对注意力强度的影响。这种方式使得模型对序列长度的变化更加鲁棒，并且能够显式地捕捉到词元间的成对关系。</p><h3 id="对比分析ape与rpe的优势与局限">2.3 对比分析：APE与RPE的优势与局限</h3><p>APE和RPE代表了两种不同的哲学思想，它们的演进为RoPE的出现奠定了基础。这场演进的背后，是模型设计中计算效率与表示能力之间根本性的张力，尤其是在“高效注意力”机制兴起的背景下，这一矛盾变得尤为突出。</p><p>APE，特别是正弦编码，其实现非常简单且计算高效。位置编码向量可以预先计算并存储，然后在模型前向传播的开始阶段与词元嵌入相加。这个过程是“可分离的”，意味着它在注意力层之外完成，并且每个词元的位置编码是独立计算的。这种特性使其与任何类型的注意力机制都能无缝兼容。然而，其主要缺点在于对相对位置的建模是间接的，并且在面对远超训练长度的序列时，其泛化能力可能会下降。</p><p>RPE通过直接修改注意力分数，为模型提供了更强大的、关于词元间相对距离的归纳偏置。这使得它在处理可变长度序列和捕捉局部上下文方面表现出色。然而，这种方法的实现方式带来了新的挑战。特别是，像T5风格的偏置方法，需要在计算注意力时访问一个依赖于相对位置的偏置矩阵，这与标准注意力机制的 <span class="math inline"><em>O</em>(<em>L</em><sup>2</sup> ⋅ <em>d</em>)</span> 复杂度和内存占用是相容的。</p><p>然而，随着模型规模和序列长度的不断增长，标准注意力机制的二次方复杂度成为一个巨大的瓶颈。为了解决这个问题，研究界开发了多种“高效注意力”或“线性注意力”方法（例如，基于核函数的方法，如Performer）。这些方法的核心思想是通过数学变换来避免显式地计算和存储完整的 <span class="math inline"><em>N</em> × <em>N</em></span> 注意力矩阵。</p><p>这就产生了一个深刻的架构冲突：RPE需要修改一个我们正极力避免去计算的注意力矩阵。如何为一个不存在的矩阵添加相对位置偏置？这个难题揭示了一个深层次的架构约束。社区迫切需要一种新的位置编码方法，它既能提供RPE的相对位置建模优势（如灵活性和泛化能力），又能保持APE的计算可分离性，从而与新兴的线性注意力架构兼容。</p><p>旋转位置编码（RoPE）正是对这一需求的直接回应。它通过在计算点积之前独立地变换查询和键向量，巧妙地将相对位置信息编码进去。这种“预处理”的方式是可分离的，因此完美地解决了上述架构冲突，统一了相对编码的强大能力和线性注意力的计算效率。</p><p>下表系统地总结了这几种主流位置编码方法的特点，突显了驱动技术向RoPE演进的核心动因。</p><table><thead><tr class="header"><th><strong>方法论</strong></th><th><strong>机制</strong></th><th><strong>位置类型</strong></th><th><strong>长度外推能力</strong></th><th><strong>与线性注意力兼容性</strong></th></tr></thead><tbody><tr class="odd"><td><strong>绝对位置编码 (APE)</strong></td><td>将位置向量<strong>加</strong>到词元嵌入上</td><td>绝对位置</td><td>理论上可行，但鲁棒性有限</td><td>良好</td></tr><tr class="even"><td><strong>相对位置编码 (RPE)</strong></td><td>将位置偏置<strong>加</strong>到注意力分数上</td><td>相对位置</td><td>良好</td><td>差（不兼容）</td></tr><tr class="odd"><td><strong>旋转位置编码 (RoPE)</strong></td><td>将词元嵌入（Q/K）与位置向量<strong>相乘</strong>（旋转）</td><td>绝对位置编码，实现相对位置依赖</td><td>优秀</td><td>良好</td></tr></tbody></table><h2 id="第三部分通往旋转位置编码之路">第三部分：通往旋转位置编码之路</h2><p>至此，我们已经完整地回顾了位置编码技术演进的脉络。我们从Transformer架构最根本的需求——克服“置换不变性”——出发，了解了最初的绝对位置编码（APE）如何通过巧妙的正弦函数设计，为模型注入了顺序感。随后，我们看到了相对位置编码（RPE）的兴起，它将关注点从“绝对位置”转向了更灵活的“相对距离”，为模型提供了更强的归纳偏置。</p><p><img src="image5.jpg" /></p><p>然而，正如我们的对比分析所揭示的，这场演进也带来了一个深刻的架构矛盾：随着模型对计算效率的要求越来越高，尤其是线性注意力机制的出现，旨在修改注意力矩阵的RPE方案变得难以兼容。一个核心问题摆在了研究者面前：<strong>我们能否创造出一种既拥有RPE相对位置建模的强大能力，又具备APE计算上的高效与分离特性的位置编码方法？</strong></p><p>这个问题的答案，正是我们下一期将要深入探讨的主角——<strong>旋转位置编码（RoPE）</strong>。</p><p>在第二期中，我们将彻底解构RoPE的迷人之处：</p><ul><li><strong>核心思想揭秘</strong>：它究竟是如何通过“旋转”而非“相加”来编码位置的？</li><li><strong>数学原理剖析</strong>：我们将深入其背后的数学公式，直观地理解它如何利用绝对位置实现相对位置的感知。</li><li><strong>关键优势解读</strong>：为什么RoPE能在序列长度外推和捕捉远程依赖方面表现如此卓越？</li><li><strong>前沿应用追踪</strong>：我们将一探究竟，看看像Llama 3和Qwen3-VL这样的顶尖模型是如何应用并优化RoPE，以解决更复杂的多模态任务的。</li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/10/27/hello-world/"/>
    <url>/2025/10/27/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
